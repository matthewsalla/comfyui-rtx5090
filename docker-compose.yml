services:
  comfyui:
    user: "${COMPOSE_UID:-1000}:${COMPOSE_GID:-1000}"
    build:
      context: .
      dockerfile: Dockerfile.comfyui
      args:
        TORCH_CHANNEL: ${TORCH_CHANNEL:-https://download.pytorch.org/whl/nightly/cu129}
        TORCH_VERSION: ${TORCH_VERSION:-}
        TORCHVISION_VERSION: ${TORCHVISION_VERSION:-}
        TORCHAUDIO_VERSION: ${TORCHAUDIO_VERSION:-}
    image: comfyui-rtx5090:latest
    container_name: comfyui-rtx5090
    restart: unless-stopped
    ports:
      - "8188:8188"
    volumes:
      - ./workspace:/workspace:rw   # single, unified mount
    environment:
      - DEBIAN_FRONTEND=noninteractive
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0

      # Use /workspace as $HOME so git & caches live in your bind mount
      - HOME=/workspace

      # Caches live under /workspace (persist & writable)
      - XDG_CACHE_HOME=/workspace/.cache/xdg
      - HF_HOME=/workspace/.cache/hf
      - TORCH_HOME=/workspace/.cache/torch
      - PIP_CACHE_DIR=/workspace/.cache/pip

      # Optional: let you override upstream at runtime
      - COMFYUI_REPO=${COMFYUI_REPO:-https://github.com/comfyanonymous/ComfyUI.git}
      - COMFYUI_BRANCH=${COMFYUI_BRANCH:-master}

      # CUDA/Torch knobs (override in .env if you want to experiment)
      - PYTORCH_CUDA_ALLOC_CONF=${PYTORCH_CUDA_ALLOC_CONF:-}
      - CUDA_MODULE_LOADING=LAZY
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_CUDNN_V8_API_ENABLED=1

      # Optional extra CLI flags for ComfyUI (e.g. "--highvram --force-fp16")
      - COMFYUI_ARGS=${COMFYUI_ARGS:-}

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 30G

    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8188"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
