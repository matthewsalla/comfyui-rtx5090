# ---------- Flash Attention wheel builder with PyTorch nightly ----------
FROM nvidia/cuda:12.9.1-cudnn-devel-ubuntu24.04

# Install Python 3.12 and dev tools
RUN apt-get update && apt-get install -y \
    python3 python3-venv python3-dev \
    gcc-12 g++-12 cmake ninja-build wget git tmux

# Create venv with Python 3.12
RUN python3 -m venv /opt/venv
ENV PATH=/opt/venv/bin:$PATH \
    TORCH_CUDA_ARCH_LIST="12.0" \
    MAX_JOBS=6 \
    MAKEFLAGS="-j6" \
    CMAKE_BUILD_PARALLEL_LEVEL=6

# Upgrade pip and install build tools
RUN pip install --upgrade pip wheel setuptools

# Reuse runtime torch channel/version for ABI compatibility
ARG TORCH_CHANNEL="https://download.pytorch.org/whl/nightly/cu129"
ARG TORCH_VERSION=""
ARG TORCHVISION_VERSION=""
ARG TORCHAUDIO_VERSION=""
RUN set -eux; \
    TORCH_SPEC="torch${TORCH_VERSION:+==${TORCH_VERSION}}"; \
    TORCHVISION_SPEC="torchvision${TORCHVISION_VERSION:+==${TORCHVISION_VERSION}}"; \
    TORCHAUDIO_SPEC="torchaudio${TORCHAUDIO_VERSION:+==${TORCHAUDIO_VERSION}}"; \
    pip install --no-cache-dir --index-url "${TORCH_CHANNEL}" "${TORCH_SPEC}"; \
    pip install --no-cache-dir --index-url "${TORCH_CHANNEL}" "${TORCHVISION_SPEC}" "${TORCHAUDIO_SPEC}"; \
    pip install einops packaging

# Build Flash Attention wheel (default 2.8.1)
ARG FLASH_ATTN_VERSION="2.8.1"
RUN pip wheel --no-binary=:all: --no-deps --no-build-isolation --no-cache-dir flash-attn==${FLASH_ATTN_VERSION} -w /wheelhouse

# List the built wheel
RUN ls -la /wheelhouse/

# Show wheel metadata to verify torch dependency
RUN pip show flash-attn || echo "flash-attn not installed, checking wheel metadata..."

# Wheel is ready in /wheelhouse/ for extraction

# Keep container running for inspection
CMD ["/bin/bash"]
